{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/jinnan/jinnan_round1_train_20181227-1.csv', encoding = 'gb18030')\n",
    "test  = pd.read_csv('../datasets/jinnan/jinnan_round1_testA_20181227.csv', encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1396 entries, 0 to 1395\n",
      "Data columns (total 44 columns):\n",
      "样本id    1396 non-null object\n",
      "A1      1396 non-null int64\n",
      "A2      42 non-null float64\n",
      "A3      1354 non-null float64\n",
      "A4      1396 non-null int64\n",
      "A5      1396 non-null object\n",
      "A6      1396 non-null float64\n",
      "A7      149 non-null object\n",
      "A8      149 non-null float64\n",
      "A9      1396 non-null object\n",
      "A10     1396 non-null int64\n",
      "A11     1396 non-null object\n",
      "A12     1396 non-null int64\n",
      "A13     1396 non-null float64\n",
      "A14     1396 non-null object\n",
      "A15     1396 non-null float64\n",
      "A16     1396 non-null object\n",
      "A17     1396 non-null float64\n",
      "A18     1396 non-null float64\n",
      "A19     1396 non-null int64\n",
      "A20     1396 non-null object\n",
      "A21     1393 non-null float64\n",
      "A22     1396 non-null float64\n",
      "A23     1393 non-null float64\n",
      "A24     1395 non-null object\n",
      "A25     1396 non-null object\n",
      "A26     1394 non-null object\n",
      "A27     1396 non-null int64\n",
      "A28     1396 non-null object\n",
      "B1      1396 non-null int64\n",
      "B2      1396 non-null float64\n",
      "B3      1396 non-null float64\n",
      "B4      1396 non-null object\n",
      "B5      1395 non-null object\n",
      "B6      1396 non-null int64\n",
      "B7      1396 non-null object\n",
      "B8      1395 non-null float64\n",
      "B9      1396 non-null object\n",
      "B10     1152 non-null object\n",
      "B11     547 non-null object\n",
      "B12     1395 non-null float64\n",
      "B13     1395 non-null float64\n",
      "B14     1396 non-null int64\n",
      "收率      1396 non-null float64\n",
      "dtypes: float64(17), int64(9), object(18)\n",
      "memory usage: 480.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A6</th>\n",
       "      <th>A8</th>\n",
       "      <th>A10</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A15</th>\n",
       "      <th>A17</th>\n",
       "      <th>A18</th>\n",
       "      <th>A19</th>\n",
       "      <th>A21</th>\n",
       "      <th>A22</th>\n",
       "      <th>A23</th>\n",
       "      <th>A27</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B6</th>\n",
       "      <th>B8</th>\n",
       "      <th>B12</th>\n",
       "      <th>B13</th>\n",
       "      <th>B14</th>\n",
       "      <th>收率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1396.000000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1393.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1393.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1395.000000</td>\n",
       "      <td>1395.000000</td>\n",
       "      <td>1395.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "      <td>1396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>298.853868</td>\n",
       "      <td>125.0</td>\n",
       "      <td>403.515510</td>\n",
       "      <td>705.974212</td>\n",
       "      <td>28.287751</td>\n",
       "      <td>78.818792</td>\n",
       "      <td>100.861032</td>\n",
       "      <td>102.641834</td>\n",
       "      <td>0.199907</td>\n",
       "      <td>103.829370</td>\n",
       "      <td>104.766905</td>\n",
       "      <td>0.199928</td>\n",
       "      <td>231.067335</td>\n",
       "      <td>48.707825</td>\n",
       "      <td>9.117120</td>\n",
       "      <td>5.002872</td>\n",
       "      <td>74.396848</td>\n",
       "      <td>334.468481</td>\n",
       "      <td>3.454477</td>\n",
       "      <td>3.500072</td>\n",
       "      <td>72.065186</td>\n",
       "      <td>43.709677</td>\n",
       "      <td>1020.215054</td>\n",
       "      <td>0.149419</td>\n",
       "      <td>410.403295</td>\n",
       "      <td>0.923244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.130552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.348093</td>\n",
       "      <td>53.214754</td>\n",
       "      <td>6.742765</td>\n",
       "      <td>2.683920</td>\n",
       "      <td>0.905198</td>\n",
       "      <td>0.915387</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.963639</td>\n",
       "      <td>1.401446</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>50.478071</td>\n",
       "      <td>4.976531</td>\n",
       "      <td>0.369152</td>\n",
       "      <td>0.136638</td>\n",
       "      <td>3.044490</td>\n",
       "      <td>104.406050</td>\n",
       "      <td>0.388310</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>9.161986</td>\n",
       "      <td>4.338396</td>\n",
       "      <td>205.920155</td>\n",
       "      <td>0.008213</td>\n",
       "      <td>26.018410</td>\n",
       "      <td>0.030880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>0.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>1.000800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                A1     A2           A3           A4           A6          A8  \\\n",
       "count  1396.000000   42.0  1354.000000  1396.000000  1396.000000  149.000000   \n",
       "mean    298.853868  125.0   403.515510   705.974212    28.287751   78.818792   \n",
       "std      10.130552    0.0    13.348093    53.214754     6.742765    2.683920   \n",
       "min     200.000000  125.0   270.000000   470.000000    17.000000   70.000000   \n",
       "25%     300.000000  125.0   405.000000   700.000000    24.000000   80.000000   \n",
       "50%     300.000000  125.0   405.000000   700.000000    29.000000   80.000000   \n",
       "75%     300.000000  125.0   405.000000   700.000000    30.000000   80.000000   \n",
       "max     300.000000  125.0   405.000000   980.000000    97.000000   82.000000   \n",
       "\n",
       "               A10          A12          A13          A15          A17  \\\n",
       "count  1396.000000  1396.000000  1396.000000  1396.000000  1396.000000   \n",
       "mean    100.861032   102.641834     0.199907   103.829370   104.766905   \n",
       "std       0.905198     0.915387     0.002524     0.963639     1.401446   \n",
       "min     100.000000    98.000000     0.120000   100.000000    89.000000   \n",
       "25%     100.000000   102.000000     0.200000   103.000000   104.000000   \n",
       "50%     101.000000   103.000000     0.200000   104.000000   105.000000   \n",
       "75%     102.000000   103.000000     0.200000   104.000000   105.000000   \n",
       "max     103.000000   107.000000     0.200000   109.000000   108.000000   \n",
       "\n",
       "               A18          A19          A21          A22          A23  \\\n",
       "count  1396.000000  1396.000000  1393.000000  1396.000000  1393.000000   \n",
       "mean      0.199928   231.067335    48.707825     9.117120     5.002872   \n",
       "std       0.002676    50.478071     4.976531     0.369152     0.136638   \n",
       "min       0.100000   100.000000    20.000000     3.500000     4.000000   \n",
       "25%       0.200000   200.000000    50.000000     9.000000     5.000000   \n",
       "50%       0.200000   200.000000    50.000000     9.000000     5.000000   \n",
       "75%       0.200000   300.000000    50.000000     9.000000     5.000000   \n",
       "max       0.200000   350.000000    90.000000    10.000000    10.000000   \n",
       "\n",
       "               A27           B1           B2           B3           B6  \\\n",
       "count  1396.000000  1396.000000  1396.000000  1396.000000  1396.000000   \n",
       "mean     74.396848   334.468481     3.454477     3.500072    72.065186   \n",
       "std       3.044490   104.406050     0.388310     0.002676     9.161986   \n",
       "min      45.000000   190.000000     0.150000     3.500000    40.000000   \n",
       "25%      73.000000   320.000000     3.500000     3.500000    65.000000   \n",
       "50%      73.000000   320.000000     3.500000     3.500000    78.000000   \n",
       "75%      77.000000   330.000000     3.500000     3.500000    80.000000   \n",
       "max      80.000000  1200.000000     3.600000     3.600000    80.000000   \n",
       "\n",
       "                B8          B12          B13          B14           收率  \n",
       "count  1395.000000  1395.000000  1395.000000  1396.000000  1396.000000  \n",
       "mean     43.709677  1020.215054     0.149419   410.403295     0.923244  \n",
       "std       4.338396   205.920155     0.008213    26.018410     0.030880  \n",
       "min      20.000000   400.000000     0.030000    40.000000     0.624000  \n",
       "25%      45.000000   800.000000     0.150000   400.000000     0.902000  \n",
       "50%      45.000000  1200.000000     0.150000   400.000000     0.925000  \n",
       "75%      45.000000  1200.000000     0.150000   420.000000     0.943000  \n",
       "max      73.000000  1200.000000     0.150000   460.000000     1.000800  "
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Unique_values</th>\n",
       "      <th>Percentage of missing values</th>\n",
       "      <th>Percentage of values in the biggest category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2</td>\n",
       "      <td>1</td>\n",
       "      <td>96.991404</td>\n",
       "      <td>96.991404</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A7</td>\n",
       "      <td>75</td>\n",
       "      <td>89.326648</td>\n",
       "      <td>89.326648</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A8</td>\n",
       "      <td>8</td>\n",
       "      <td>89.326648</td>\n",
       "      <td>89.326648</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>B11</td>\n",
       "      <td>37</td>\n",
       "      <td>60.816619</td>\n",
       "      <td>60.816619</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>B10</td>\n",
       "      <td>180</td>\n",
       "      <td>17.478510</td>\n",
       "      <td>17.478510</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.008596</td>\n",
       "      <td>95.702006</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>99.641834</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A21</td>\n",
       "      <td>12</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>89.828080</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A26</td>\n",
       "      <td>88</td>\n",
       "      <td>0.143266</td>\n",
       "      <td>18.982808</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>B8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.071633</td>\n",
       "      <td>77.722063</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  Unique_values  Percentage of missing values  \\\n",
       "2       A2              1                     96.991404   \n",
       "7       A7             75                     89.326648   \n",
       "8       A8              8                     89.326648   \n",
       "39     B11             37                     60.816619   \n",
       "38     B10            180                     17.478510   \n",
       "3       A3              3                      3.008596   \n",
       "23     A23              3                      0.214900   \n",
       "21     A21             12                      0.214900   \n",
       "26     A26             88                      0.143266   \n",
       "36      B8             25                      0.071633   \n",
       "\n",
       "    Percentage of values in the biggest category     type  \n",
       "2                                      96.991404  float64  \n",
       "7                                      89.326648   object  \n",
       "8                                      89.326648  float64  \n",
       "39                                     60.816619   object  \n",
       "38                                     17.478510   object  \n",
       "3                                      95.702006  float64  \n",
       "23                                     99.641834  float64  \n",
       "21                                     89.828080  float64  \n",
       "26                                     18.982808   object  \n",
       "36                                     77.722063  float64  "
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = []\n",
    "for col in train.columns:\n",
    "    stats.append((col, train[col].nunique(), train[col].isnull().sum() * 100 / train.shape[0], train[col].value_counts(normalize=True, dropna=False).values[0] * 100, train[col].dtype))\n",
    "    \n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Unique_values</th>\n",
       "      <th>Percentage of missing values</th>\n",
       "      <th>Percentage of values in the biggest category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2</td>\n",
       "      <td>1</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A7</td>\n",
       "      <td>15</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A8</td>\n",
       "      <td>2</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>B11</td>\n",
       "      <td>11</td>\n",
       "      <td>59.333333</td>\n",
       "      <td>59.333333</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>B10</td>\n",
       "      <td>50</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A27</td>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>47.333333</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A20</td>\n",
       "      <td>52</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>B1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  Unique_values  Percentage of missing values  \\\n",
       "2       A2              1                     98.000000   \n",
       "7       A7             15                     88.000000   \n",
       "8       A8              2                     88.000000   \n",
       "39     B11             11                     59.333333   \n",
       "38     B10             50                     14.666667   \n",
       "3       A3              2                      2.000000   \n",
       "25     A25              7                      0.666667   \n",
       "27     A27             11                      0.666667   \n",
       "20     A20             52                      0.666667   \n",
       "29      B1             13                      0.666667   \n",
       "\n",
       "    Percentage of values in the biggest category     type  \n",
       "2                                      98.000000  float64  \n",
       "7                                      88.000000   object  \n",
       "8                                      88.000000  float64  \n",
       "39                                     59.333333   object  \n",
       "38                                     14.666667   object  \n",
       "3                                      97.333333  float64  \n",
       "25                                     42.000000  float64  \n",
       "27                                     47.333333  float64  \n",
       "20                                     20.000000   object  \n",
       "29                                     56.000000  float64  "
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = []\n",
    "for col in test.columns:\n",
    "    stats.append((col, test[col].nunique(), test[col].isnull().sum() * 100 / test.shape[0], test[col].value_counts(normalize=True, dropna=False).values[0] * 100, test[col].dtype))\n",
    "    \n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAF6CAYAAAD4TjrrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+CVJqhCDX5KkCvn/NjvZVR7/roQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec36a7c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_col = \"收率\"\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train.shape[0]), np.sort(train[target_col].values))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('yield', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHzCAYAAADSPIC7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgCAgUgGAIDB/wMHZ0+kJsFPTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec36a7c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train[target_col].values, bins=50, kde=False, color=\"red\")\n",
    "plt.title(\"Histogram of yield\")\n",
    "plt.xlabel('yield', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train, test]:\n",
    "    df.drop(['B3', 'B13', 'A13', 'A18', 'A23'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 0.9863896848137536\n",
      "A2 0.9699140401146131\n",
      "A3 0.9570200573065902\n",
      "A4 0.9570200573065902\n",
      "B2 0.9856733524355301\n"
     ]
    }
   ],
   "source": [
    "# 删除某一类别占比超过90%的列\n",
    "good_cols = list(train.columns)\n",
    "for col in train.columns:\n",
    "    rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.9:\n",
    "        good_cols.remove(col)\n",
    "        print(col,rate)\n",
    "\n",
    "# 暂时不删除，后面构造特征需要\n",
    "good_cols.append('A4')\n",
    "\n",
    "# 删除异常值\n",
    "train = train[train['收率']>0.87]\n",
    "        \n",
    "train = train[good_cols]\n",
    "good_cols.remove('收率')\n",
    "test  = test[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "target = train['收率']\n",
    "del train['收率']\n",
    "data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeTranSecond(t):\n",
    "    try:\n",
    "        t,m,s=t.split(\":\")\n",
    "    except:\n",
    "        if t=='1900/1/9 7:00':\n",
    "            return 7*3600/3600\n",
    "        elif t=='1900/1/1 2:30':\n",
    "            return (2*3600+30*60)/3600\n",
    "        elif t==-1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    try:\n",
    "        tm = (int(t)*3600+int(m)*60+int(s))/3600\n",
    "    except:\n",
    "        return (30*60)/3600\n",
    "    \n",
    "    return tm\n",
    "for f in ['A5','A7','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "    try:\n",
    "        data[f] = data[f].apply(timeTranSecond)\n",
    "    except:\n",
    "        print(f,'应该在前面被删除了！')\n",
    "\n",
    "def getDuration(se):\n",
    "    try:\n",
    "        sh,sm,eh,em=re.findall(r\"\\d+\\.?\\d*\",se)\n",
    "    except:\n",
    "        if se == -1:\n",
    "            return -1 \n",
    "        \n",
    "    try:\n",
    "        if int(sh)>int(eh):\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "        else:\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "    except:\n",
    "        if se=='19:-20:05':\n",
    "            return 1\n",
    "        elif se=='15:00-1600':\n",
    "            return 1\n",
    "    \n",
    "    return tm\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f] = data.apply(lambda df: getDuration(df[f]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['样本id'] = data['样本id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "categorical_columns = [f for f in data.columns if f not in ['样本id']]\n",
    "numerical_columns = [f for f in data.columns if f not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['样本id', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A14', 'A15',\n",
       "       'A16', 'A17', 'A19', 'A20', 'A21', 'A22', 'A24', 'A25', 'A26', 'A27',\n",
       "       'A28', 'B1', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11', 'B12',\n",
       "       'B14', 'b14/a4_a19_a21_a22_b1_b12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 催化剂的浓度\n",
    "data['b14/a4_a19_a21_a22_b1_b12'] = data['B14']/(data['A4']+data['A19']+data['A21']+data['A22']+data['B1']+data['B12']+data['B14'])\n",
    "# data['b12/a4_a19_a21_a22_b1'] = data['B12']/(data['A4']+data['A19']+data['A21']+data['A22']+data['B1']+data['B12'])\n",
    "# data['b1/a4_a19_a21_a22'] = data['B1']/(data['A4']+data['A19']+data['A21']+data['A22']+data['B1'])\n",
    "# data['a21_a22/a4_a19'] = (data['A21']+data['A22'])/(data['A4']+data['A19']+data['A21']+data['A22'])\n",
    "# data['a19/a4'] = data['A19']/(data['A4']+data['A19'])\n",
    "\n",
    "numerical_columns.append('b14/a4_a19_a21_a22_b1_b12')\n",
    "# numerical_columns.append('b12/a4_a19_a21_a22_b1')\n",
    "# numerical_columns.append('b1/a4_a19_a21_a22')\n",
    "# numerical_columns.append('a21_a22/a4_a19')\n",
    "# numerical_columns.append('a19/a4')\n",
    "\n",
    "del data['A4']\n",
    "categorical_columns.remove('A4')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 34)\n",
      "(150, 34)\n"
     ]
    }
   ],
   "source": [
    "#label encoder\n",
    "for f in categorical_columns:\n",
    "    data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "train = data[:train.shape[0]]\n",
    "test  = data[train.shape[0]:]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加新特征，将收率进行分箱，然后构造每个特征中的类别对应不同收率的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 124)\n",
      "(150, 124)\n"
     ]
    }
   ],
   "source": [
    "#train['target'] = list(target) \n",
    "train['target'] = target\n",
    "train['intTarget'] = pd.cut(train['target'], 6, labels=False)\n",
    "train = pd.get_dummies(train, columns=['intTarget'])\n",
    "li = ['intTarget_0.0','intTarget_1.0','intTarget_2.0','intTarget_3.0','intTarget_4.0','intTarget_5.0']\n",
    "mean_columns = []\n",
    "\n",
    "for f1 in categorical_columns:\n",
    "    cate_rate = train[f1].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if cate_rate < 0.90:\n",
    "        for f2 in li:\n",
    "            col_name = 'B14_to_'+f1+\"_\"+f2+'_mean'\n",
    "            mean_columns.append(col_name)\n",
    "            order_label = train.groupby([f1])[f2].mean()\n",
    "            train[col_name] = train['B14'].map(order_label)\n",
    "            miss_rate = train[col_name].isnull().sum() * 100 / train[col_name].shape[0]\n",
    "            if miss_rate > 0:\n",
    "                train = train.drop([col_name], axis=1)\n",
    "                mean_columns.remove(col_name)\n",
    "            else:\n",
    "                test[col_name] = test['B14'].map(order_label)\n",
    "\n",
    "\n",
    "train.drop(li+['target'], axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modeling_cross_validation(params, X, y, nr_folds=5):\n",
    "    \n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    # Split data with kfold\n",
    "    folds = KFold(n_splits=nr_folds, shuffle=False, random_state=4096)\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "        val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "\n",
    "        num_round = 20000\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n",
    "        oof_preds[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "    score = mean_squared_error(oof_preds, target)\n",
    "    \n",
    "    return  score/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSelect(init_cols):\n",
    "    params = {'num_leaves': 120,\n",
    "             'min_data_in_leaf': 30, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.05,\n",
    "             \"min_child_samples\": 30,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'mse',\n",
    "             \"lambda_l1\": 0.02,\n",
    "             \"verbosity\": -1}\n",
    "    best_cols = init_cols.copy()\n",
    "    best_score = modeling_cross_validation(params, train[init_cols].values, target.values, nr_folds=5)\n",
    "    print(\"初始CV score: {:<8.8f}\".format(best_score))\n",
    "    for f in init_cols:\n",
    "\n",
    "        best_cols.remove(f)\n",
    "        score = modeling_cross_validation(params, train[best_cols].values, target.values, nr_folds=5)\n",
    "        diff = best_score - score\n",
    "        print('-'*10)\n",
    "        if diff > 0.0000002:\n",
    "            print(\"当前移除特征: {}, CV score: {:<8.8f}, 最佳cv score: {:<8.8f}, 有效果,删除！！\".format(f,score,best_score))\n",
    "            best_score = score\n",
    "        else:\n",
    "            print(\"当前移除特征: {}, CV score: {:<8.8f}, 最佳cv score: {:<8.8f}, 没效果,保留！！\".format(f,score,best_score))\n",
    "            best_cols.append(f)\n",
    "    print('-'*10)\n",
    "    print(\"优化后CV score: {:<8.8f}\".format(best_score))\n",
    "    \n",
    "    return best_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 1260)\n",
      "(150, 1260)\n"
     ]
    }
   ],
   "source": [
    "# best_features = featureSelect(train.columns.tolist())\n",
    "# print(best_features)\n",
    "\n",
    "X_train = train[mean_columns+numerical_columns].values\n",
    "X_test = test[mean_columns+numerical_columns].fillna(0).values\n",
    "# one hot\n",
    "enc = OneHotEncoder()\n",
    "for f in categorical_columns:\n",
    "    enc.fit(data[f].values.reshape(-1, 1))\n",
    "    X_train = sparse.hstack((X_train, enc.transform(train[f].values.reshape(-1, 1))), 'csr')\n",
    "    X_test = sparse.hstack((X_test, enc.transform(test[f].values.reshape(-1, 1))), 'csr')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000213028\tvalid_1's l2: 0.00023346\n",
      "[400]\ttraining's l2: 0.000139855\tvalid_1's l2: 0.000159986\n",
      "[600]\ttraining's l2: 0.000113944\tvalid_1's l2: 0.000136801\n",
      "[800]\ttraining's l2: 0.00010339\tvalid_1's l2: 0.000128794\n",
      "[1000]\ttraining's l2: 9.78691e-05\tvalid_1's l2: 0.000125413\n",
      "[1200]\ttraining's l2: 9.42577e-05\tvalid_1's l2: 0.000123144\n",
      "[1400]\ttraining's l2: 9.16196e-05\tvalid_1's l2: 0.000121707\n",
      "[1600]\ttraining's l2: 8.9649e-05\tvalid_1's l2: 0.000120889\n",
      "[1800]\ttraining's l2: 8.79144e-05\tvalid_1's l2: 0.000120043\n",
      "[2000]\ttraining's l2: 8.65447e-05\tvalid_1's l2: 0.000119689\n",
      "[2200]\ttraining's l2: 8.53362e-05\tvalid_1's l2: 0.000119266\n",
      "[2400]\ttraining's l2: 8.42968e-05\tvalid_1's l2: 0.000118849\n",
      "[2600]\ttraining's l2: 8.3326e-05\tvalid_1's l2: 0.000118568\n",
      "Early stopping, best iteration is:\n",
      "[2680]\ttraining's l2: 8.29964e-05\tvalid_1's l2: 0.00011839\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000209282\tvalid_1's l2: 0.000245488\n",
      "[400]\ttraining's l2: 0.000136577\tvalid_1's l2: 0.000175193\n",
      "[600]\ttraining's l2: 0.000113118\tvalid_1's l2: 0.000152353\n",
      "[800]\ttraining's l2: 0.000102919\tvalid_1's l2: 0.000144288\n",
      "[1000]\ttraining's l2: 9.69491e-05\tvalid_1's l2: 0.000140213\n",
      "[1200]\ttraining's l2: 9.29514e-05\tvalid_1's l2: 0.000137989\n",
      "[1400]\ttraining's l2: 9.02256e-05\tvalid_1's l2: 0.000136293\n",
      "[1600]\ttraining's l2: 8.80399e-05\tvalid_1's l2: 0.000134945\n",
      "[1800]\ttraining's l2: 8.63904e-05\tvalid_1's l2: 0.000133989\n",
      "[2000]\ttraining's l2: 8.49553e-05\tvalid_1's l2: 0.000133404\n",
      "[2200]\ttraining's l2: 8.36608e-05\tvalid_1's l2: 0.000132811\n",
      "[2400]\ttraining's l2: 8.25531e-05\tvalid_1's l2: 0.000132313\n",
      "Early stopping, best iteration is:\n",
      "[2476]\ttraining's l2: 8.21627e-05\tvalid_1's l2: 0.00013208\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000212711\tvalid_1's l2: 0.00021879\n",
      "[400]\ttraining's l2: 0.00014067\tvalid_1's l2: 0.000164479\n",
      "[600]\ttraining's l2: 0.00011374\tvalid_1's l2: 0.000145145\n",
      "[800]\ttraining's l2: 0.000103298\tvalid_1's l2: 0.000138263\n",
      "[1000]\ttraining's l2: 9.77646e-05\tvalid_1's l2: 0.000135\n",
      "[1200]\ttraining's l2: 9.41479e-05\tvalid_1's l2: 0.000133172\n",
      "[1400]\ttraining's l2: 9.15977e-05\tvalid_1's l2: 0.000131565\n",
      "[1600]\ttraining's l2: 8.96574e-05\tvalid_1's l2: 0.000130501\n",
      "[1800]\ttraining's l2: 8.80203e-05\tvalid_1's l2: 0.000129698\n",
      "[2000]\ttraining's l2: 8.67479e-05\tvalid_1's l2: 0.000128978\n",
      "[2200]\ttraining's l2: 8.56667e-05\tvalid_1's l2: 0.000128402\n",
      "[2400]\ttraining's l2: 8.47152e-05\tvalid_1's l2: 0.000127871\n",
      "[2600]\ttraining's l2: 8.38808e-05\tvalid_1's l2: 0.000127428\n",
      "[2800]\ttraining's l2: 8.3416e-05\tvalid_1's l2: 0.000127229\n",
      "Early stopping, best iteration is:\n",
      "[2714]\ttraining's l2: 8.343e-05\tvalid_1's l2: 0.000127221\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000209564\tvalid_1's l2: 0.000229008\n",
      "[400]\ttraining's l2: 0.000137045\tvalid_1's l2: 0.000172567\n",
      "[600]\ttraining's l2: 0.000109631\tvalid_1's l2: 0.000152646\n",
      "[800]\ttraining's l2: 9.98161e-05\tvalid_1's l2: 0.000147005\n",
      "[1000]\ttraining's l2: 9.43076e-05\tvalid_1's l2: 0.000144268\n",
      "[1200]\ttraining's l2: 9.06755e-05\tvalid_1's l2: 0.000142977\n",
      "[1400]\ttraining's l2: 8.78909e-05\tvalid_1's l2: 0.000141975\n",
      "[1600]\ttraining's l2: 8.59132e-05\tvalid_1's l2: 0.000141302\n",
      "[1800]\ttraining's l2: 8.42314e-05\tvalid_1's l2: 0.000141033\n",
      "[2000]\ttraining's l2: 8.27602e-05\tvalid_1's l2: 0.000140616\n",
      "[2200]\ttraining's l2: 8.15176e-05\tvalid_1's l2: 0.000140431\n",
      "Early stopping, best iteration is:\n",
      "[2178]\ttraining's l2: 8.16335e-05\tvalid_1's l2: 0.000140357\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000214914\tvalid_1's l2: 0.000215243\n",
      "[400]\ttraining's l2: 0.000141377\tvalid_1's l2: 0.000147411\n",
      "[600]\ttraining's l2: 0.000116133\tvalid_1's l2: 0.000126085\n",
      "[800]\ttraining's l2: 0.000105942\tvalid_1's l2: 0.000118876\n",
      "[1000]\ttraining's l2: 0.000100072\tvalid_1's l2: 0.000115462\n",
      "[1200]\ttraining's l2: 9.59672e-05\tvalid_1's l2: 0.000113352\n",
      "[1400]\ttraining's l2: 9.31207e-05\tvalid_1's l2: 0.000112204\n",
      "[1600]\ttraining's l2: 9.09113e-05\tvalid_1's l2: 0.00011136\n",
      "[1800]\ttraining's l2: 8.90369e-05\tvalid_1's l2: 0.000110903\n",
      "[2000]\ttraining's l2: 8.74584e-05\tvalid_1's l2: 0.000110664\n",
      "[2200]\ttraining's l2: 8.61166e-05\tvalid_1's l2: 0.000110369\n",
      "[2400]\ttraining's l2: 8.49432e-05\tvalid_1's l2: 0.000110261\n",
      "[2600]\ttraining's l2: 8.38607e-05\tvalid_1's l2: 0.0001101\n",
      "Early stopping, best iteration is:\n",
      "[2629]\ttraining's l2: 8.37083e-05\tvalid_1's l2: 0.000110079\n",
      "fold n°6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000212659\tvalid_1's l2: 0.000231656\n",
      "[400]\ttraining's l2: 0.000142609\tvalid_1's l2: 0.000153828\n",
      "[600]\ttraining's l2: 0.000115324\tvalid_1's l2: 0.000123757\n",
      "[800]\ttraining's l2: 0.000105025\tvalid_1's l2: 0.000115317\n",
      "[1000]\ttraining's l2: 9.94955e-05\tvalid_1's l2: 0.000111558\n",
      "[1200]\ttraining's l2: 9.56617e-05\tvalid_1's l2: 0.000108984\n",
      "[1400]\ttraining's l2: 9.29558e-05\tvalid_1's l2: 0.000107541\n",
      "[1600]\ttraining's l2: 9.08022e-05\tvalid_1's l2: 0.000106476\n",
      "[1800]\ttraining's l2: 8.9057e-05\tvalid_1's l2: 0.000105639\n",
      "[2000]\ttraining's l2: 8.76917e-05\tvalid_1's l2: 0.000105206\n",
      "[2200]\ttraining's l2: 8.63957e-05\tvalid_1's l2: 0.000104745\n",
      "[2400]\ttraining's l2: 8.52335e-05\tvalid_1's l2: 0.000104289\n",
      "[2600]\ttraining's l2: 8.42442e-05\tvalid_1's l2: 0.000103735\n",
      "[2800]\ttraining's l2: 8.35563e-05\tvalid_1's l2: 0.000103418\n",
      "Early stopping, best iteration is:\n",
      "[2757]\ttraining's l2: 8.35656e-05\tvalid_1's l2: 0.000103415\n",
      "CV score: 0.00012192\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.008,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'mse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "# max_depth < 0 模型深度没限制\n",
    "\n",
    "folds = KFold(n_splits=6, shuffle=True, random_state=2018)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:0.421058\tvalid_data-rmse:0.420683\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155494\tvalid_data-rmse:0.15515\n",
      "[200]\ttrain-rmse:0.058683\tvalid_data-rmse:0.058926\n",
      "[300]\ttrain-rmse:0.023766\tvalid_data-rmse:0.024878\n",
      "[400]\ttrain-rmse:0.01149\tvalid_data-rmse:0.014215\n",
      "[500]\ttrain-rmse:0.007499\tvalid_data-rmse:0.011749\n",
      "[600]\ttrain-rmse:0.006177\tvalid_data-rmse:0.011293\n",
      "[700]\ttrain-rmse:0.005532\tvalid_data-rmse:0.011196\n",
      "[800]\ttrain-rmse:0.005029\tvalid_data-rmse:0.011179\n",
      "[900]\ttrain-rmse:0.00462\tvalid_data-rmse:0.011179\n",
      "[1000]\ttrain-rmse:0.004284\tvalid_data-rmse:0.011173\n",
      "Stopping. Best iteration:\n",
      "[822]\ttrain-rmse:0.004937\tvalid_data-rmse:0.011166\n",
      "\n",
      "fold n°2\n",
      "[0]\ttrain-rmse:0.421387\tvalid_data-rmse:0.419022\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155662\tvalid_data-rmse:0.155234\n",
      "[200]\ttrain-rmse:0.058744\tvalid_data-rmse:0.058706\n",
      "[300]\ttrain-rmse:0.023704\tvalid_data-rmse:0.024869\n",
      "[400]\ttrain-rmse:0.011358\tvalid_data-rmse:0.014397\n",
      "[500]\ttrain-rmse:0.007406\tvalid_data-rmse:0.011978\n",
      "[600]\ttrain-rmse:0.00609\tvalid_data-rmse:0.011508\n",
      "[700]\ttrain-rmse:0.00547\tvalid_data-rmse:0.011419\n",
      "[800]\ttrain-rmse:0.005038\tvalid_data-rmse:0.01139\n",
      "[900]\ttrain-rmse:0.004659\tvalid_data-rmse:0.01139\n",
      "[1000]\ttrain-rmse:0.004291\tvalid_data-rmse:0.011393\n",
      "Stopping. Best iteration:\n",
      "[874]\ttrain-rmse:0.004746\tvalid_data-rmse:0.011379\n",
      "\n",
      "fold n°3\n",
      "[0]\ttrain-rmse:0.421024\tvalid_data-rmse:0.420879\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155465\tvalid_data-rmse:0.15604\n",
      "[200]\ttrain-rmse:0.05874\tvalid_data-rmse:0.059596\n",
      "[300]\ttrain-rmse:0.023777\tvalid_data-rmse:0.024829\n",
      "[400]\ttrain-rmse:0.011491\tvalid_data-rmse:0.013832\n",
      "[500]\ttrain-rmse:0.007433\tvalid_data-rmse:0.011383\n",
      "[600]\ttrain-rmse:0.006022\tvalid_data-rmse:0.011005\n",
      "[700]\ttrain-rmse:0.005388\tvalid_data-rmse:0.010974\n",
      "[800]\ttrain-rmse:0.004884\tvalid_data-rmse:0.010994\n",
      "Stopping. Best iteration:\n",
      "[664]\ttrain-rmse:0.00558\tvalid_data-rmse:0.01097\n",
      "\n",
      "fold n°4\n",
      "[0]\ttrain-rmse:0.420911\tvalid_data-rmse:0.4214\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155478\tvalid_data-rmse:0.155226\n",
      "[200]\ttrain-rmse:0.058756\tvalid_data-rmse:0.058234\n",
      "[300]\ttrain-rmse:0.023786\tvalid_data-rmse:0.023483\n",
      "[400]\ttrain-rmse:0.011493\tvalid_data-rmse:0.013238\n",
      "[500]\ttrain-rmse:0.007528\tvalid_data-rmse:0.01139\n",
      "[600]\ttrain-rmse:0.006225\tvalid_data-rmse:0.01124\n",
      "[700]\ttrain-rmse:0.005577\tvalid_data-rmse:0.011283\n",
      "[800]\ttrain-rmse:0.005107\tvalid_data-rmse:0.011358\n",
      "Stopping. Best iteration:\n",
      "[613]\ttrain-rmse:0.006107\tvalid_data-rmse:0.011231\n",
      "\n",
      "fold n°5\n",
      "[0]\ttrain-rmse:0.420518\tvalid_data-rmse:0.423345\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155319\tvalid_data-rmse:0.156431\n",
      "[200]\ttrain-rmse:0.058708\tvalid_data-rmse:0.059322\n",
      "[300]\ttrain-rmse:0.023802\tvalid_data-rmse:0.024438\n",
      "[400]\ttrain-rmse:0.011546\tvalid_data-rmse:0.013318\n",
      "[500]\ttrain-rmse:0.007522\tvalid_data-rmse:0.010734\n",
      "[600]\ttrain-rmse:0.006129\tvalid_data-rmse:0.010348\n",
      "[700]\ttrain-rmse:0.00546\tvalid_data-rmse:0.010354\n",
      "[800]\ttrain-rmse:0.00501\tvalid_data-rmse:0.010408\n",
      "Stopping. Best iteration:\n",
      "[646]\ttrain-rmse:0.00579\tvalid_data-rmse:0.010341\n",
      "\n",
      "fold n°6\n",
      "[0]\ttrain-rmse:0.421067\tvalid_data-rmse:0.420628\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.155502\tvalid_data-rmse:0.154913\n",
      "[200]\ttrain-rmse:0.05877\tvalid_data-rmse:0.058151\n",
      "[300]\ttrain-rmse:0.023798\tvalid_data-rmse:0.024262\n",
      "[400]\ttrain-rmse:0.011538\tvalid_data-rmse:0.013519\n",
      "[500]\ttrain-rmse:0.007561\tvalid_data-rmse:0.010968\n",
      "[600]\ttrain-rmse:0.006199\tvalid_data-rmse:0.010462\n",
      "[700]\ttrain-rmse:0.005582\tvalid_data-rmse:0.010376\n",
      "[800]\ttrain-rmse:0.0051\tvalid_data-rmse:0.010364\n",
      "[900]\ttrain-rmse:0.004708\tvalid_data-rmse:0.010381\n",
      "[1000]\ttrain-rmse:0.004371\tvalid_data-rmse:0.010402\n",
      "Stopping. Best iteration:\n",
      "[823]\ttrain-rmse:0.005014\tvalid_data-rmse:0.01036\n",
      "\n",
      "CV score: 0.00011916\n"
     ]
    }
   ],
   "source": [
    "##### xgb\n",
    "xgb_params = {'eta': 0.01, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 8}\n",
    "\n",
    "# reg:linear 线性回归；reg:logistic 逻辑回归\n",
    "\n",
    "folds = KFold(n_splits=6, shuffle=True, random_state=2018)\n",
    "oof_xgb = np.zeros(len(train))\n",
    "predictions_xgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(X_train[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    \n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n",
      "fold 10\n",
      "fold 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00011254471318199566"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将lgb和xgb的结果进行stacking\n",
    "train_stack = np.vstack([oof_lgb, oof_xgb]).transpose()\n",
    "test_stack = np.vstack([predictions_lgb, predictions_xgb]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=6, n_repeats=2, random_state=4590)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = GradientBoostingRegressor(n_estimators=90, learning_rate=0.08, max_depth=2, random_state=0, loss='ls')\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions += clf_3.predict(test_stack) / 12\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../datasets/jinnan/jinnan_round1_submit_20181227.csv', header=None)\n",
    "sub_df[1] = predictions\n",
    "sub_df[1] = sub_df[1].apply(lambda x:round(x, 3))\n",
    "sub_df.to_csv(\"../datasets/jinnan/results.csv\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
